<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title> | Oliver Stollmann's sudo rm -rf /</title>
    <link href="http://fonts.googleapis.com/css?family=Inconsolata" rel="stylesheet" type="text/css">
    <link href="http://o1iver.net/media/pygments.css" rel="stylesheet" type="text/css">
    <link href="/static/style.css" rel="stylesheet" type="text/css">
    <link rel="alternate" href="/feed.atom" title="Recent Blog Posts" type="application/atom+xml">
    <link rel="stylesheet" href="/static/_pygments.css" type="text/css">
</head>

<body>
    <div class="container">
        <div class="header">
            <span class="blogtitle">Oliver's Backward Induction</span>
        </div>
        <div class="navigation">[
            <a href="/">blog</a>, 
            <a href="/archive/">archive</a>, 
            <a href="/tags/">tags</a>,
            <a href="/feed.atom" rel="alternate" title="Oliver's Backward Induction - Recent Posts">feed</a>, 
            <a href="http://o1iver.net/">homepage</a>]
        </div>
        <div class="body">
  
  
  <p class=date>written on Saturday, November 12, 2011
  

  <div class="section" id="markov-chain-text-generator">
<h2>Markov Chain Text Generator</h2>
<p>The topic of my bachelor thesis (work in progress) is &quot;Markov Decision Processes for Optimal Control (of Waste-to-Energy Plants)&quot;. This is a super interesting topic and I am having lots of fun playing with MDPs. MDPs are actually an extension of Markov Chains. I am not going to go through Markov Chains in great detail, but there are two things that must be said.</p>
<div class="section" id="markov-property">
<h3>Markov Property</h3>
<p>The Markov property states, that future states only depend on the current state. If the Markov Property holds for a certain stochastic process you can say, that the process is &quot;memoryless&quot;!</p>
</div>
<div class="section" id="transition-probabilities">
<h3>Transition Probabilities</h3>
<p>A transition probability describes the likelyhood of going from a certain state to another state (in the next epoch). Consider the following two state example:</p>
<div class="highlight"><pre>- State 1: relationship status is <span class="s1">&#39;single&#39;</span>
- State 2: relationship status is <span class="s1">&#39;in a relationship&#39;</span>
- State 3: relationship status is <span class="s1">&#39;engaged&#39;</span>
- State 4: relationship status is <span class="s1">&#39;married&#39;</span>
- State 5: relationship status is <span class="s1">&#39;divorced&#39;</span>
</pre></div>
<p>Now the transition probabilities (they are represented as a matrix):</p>
<div class="highlight"><pre>              s i n k
s |-----------------------------|
o | 0.7 | 0.3 | 0.0 | 0.0 | 0.0 |
u | 0.2 | 0.7 | 0.1 | 0.0 | 0.0 |
r | 0.1 | 0.0 | 0.2 | 0.7 | 0.0 |
c | 0.0 | 0.1 | 0.0 | 0.7 | 0.2 |
e | 0.0 | 0.3 | 0.0.| 0.0 | 0.7 |
  |-----------------------------|
</pre></div>
<p>So what does this mean? Well the rows correspond to source states (current state) and the columns to sink states (next state), and the numbers represent the probability of transitioning from the given source to the given sink state. So for example, let's look at the second row. Being in this row means that the person this represents is in a relationship. The numbers in the row define the probability of which state will be reached at the next epoch. So for example the 0.2 in the first column means that there is a 20% probability of going from being in a relationship to being single again. Or the last column: the probability of going to state 'divorced' is 0.0. This makes sense as it is impossible to become divorced if you are not married yet!</p>
<p><em>I hope that made sense.</em></p>
</div>
</div>
<div class="section" id="text-generation-using-markov-chains">
<h2>Text Generation using Markov Chains</h2>
<p>So now come the fun part. Firstly: how in the world is this stuff going to help me generate text? Well actually it is very simple. What we do is we say that words are states and transition probabilities describe the probabilty of a certain word following another word. What this means is that we can use a certain training text to extract the probabilities of for example the word 'world' following the word 'hello'. Does that make sense? Let me repeat that to be clear. We use a training text to build a transition probability matrix, which we can then use to generate a text by using these <em>word A follows word B</em> probabilities.</p>
<p><em>Note: when I say word, I actually mean token, so even exclamation marks and commas count as word in this context.</em></p>
<p>To give you an example of a generated text. Here is a text (1000 words) that I generated using a transition probability matrix extracted from the borther Grimms' fairy tales:</p>
<p><em>The countryman.</em></p>
<p><em>The two oxen a good children.! Cried he took the wind, and trotted away about the money upon the boar ran about, and watched for himself down and crept into the horse and had eaten this is all was about, took my neighbours, took his road; They went the forest. So beautiful roses that you here she would like to be managed to set up than old ones. Their brother fortune would do not one evening, away, and try what have a fine fat goose, who were tired;</em></p>
<p><em>The ale that it was, just before the well taken from my wife said the youngest said, and when he tasted very easily get me? Said sorrowfully. Then said she did they could hardly hold your wife were not straight over upon it, so that he met with me into the monster stood on her great grief came nearer they should be black friend? Asked him into a golden goblet full of your body, and his time of life as they came and guess what we were discovered in the third night, and said, if he should give them all had once again asked the bottle, gretel. The same time and they were too wished himself. But the little dwarf should be away curdken ran into my heart, said the cat said the stick, and cried the maiden, who had flown out. And the miller wife was once more beautiful ring as she cut down on the promised to the way home, and when she had enough yet. He is tired him: the ground. But gretel, like to sleep for the king said, gretel thought: so whilst they set to draw beer. Then he consented, and the walls; So he was, and it; And shall not gone; Whom everyone wondered if you go a little way when he died of gold? Asked in the cupboard, and out at length agreed to a young girl remained sticking fast. In upon the young lady is only said he lifted its liberty, bury my brown, and dug his mind and rolled on her head that the forest dwelt, hans.</em></p>
<p><em>So frightened me! The queen went down and that at him. Have worked and rode up. Wo you may get rid of him and he did not yourself, till she. Think, answered: a strong, he jogged, and overset the miller, old song was to the salad ready, and waited a dove?</em></p>
<p><em>She was thrown at last a handful of the four years had said the sexton when he should at the village, no evil would, my order that he answered the middle of the juniper-tree kywitt, and was rich, whose lives here?</em></p>
<p><em>Asked what have been made her, who asked he called out of no one side of any longer the ring, and kicked him to the hill and said he married king son said:, and ask her own house, and spun gold, they saw your little friend again this frightened, he was jumping about what she flew away; Give you no one blow, and wait until that he could want you old as day, and I to take them ran catherine, would? Said, darting. Then went inside it again.</em></p>
<p><em>The old song was carried away one of gold; The lion, but devoured her say, shall not believe that he named gretel presents hans took my end to sit and eat, would be able to give them on jollily together, I am beginning to do or seen the prince said the other, she walked into bed.</em></p>
<p><em>It seemed as in one day, and we are you seek for it, just tell no injury. The juniper-tree stood all three hundred paces to him. Let her.</em></p>
<p><em>The merry and once more than money in every hole where they are the parson house. The other after all the princess was always stand godmother; And he was bewitched by the king said the dog. These should have got up the musicians an end of spirits, when it was again over him.</em></p>
<p><em>Then another of cloth it happened before the forest, and opened. The house: and then the old nail in the power to the murderers den, little dwarf met them out with you, and the twenty of the feast, and said to run and would not see what is cast into the water the second son. In the morning just then they were some more glad indeed! Said her up she began to the cat entirely emptied the fox with the sea! Said, the water.</em></p>
<p><em>The way over my face and the town, all his money to awaken him once more piece of cold and next day more pipe and put her best way, mr korbes came and sit still, she came close by me, asked how falada again. The gold, like big that everyone called all went back and saw that nothing but the wind, and the well married; I saw them as you get out my map, we will give a beautiful sleeping on a shame that she, the king, and expected to earn his teeth?</em></p>
<p><em>In a draught, and as he went over upon that you the same day frederick and that the goat who was glad, I shall die, and will go.</em></p>
</div>
<div class="section" id="generator-code">
<h2>Generator Code</h2>
<p>The code is available at <a class="reference external" href="http://github.com/o1iver/markov-text-generator">http://github.com/o1iver/markov-text-generator</a>. To generate text run the following command:</p>
<div class="highlight"><pre>python mg.py &lt;training-text-file&gt; &lt;output-file&gt; &lt;number-of-words&gt; &lt;num-states-factor&gt;
</pre></div>
<p>Example:</p>
<div class="highlight"><pre>python mg.py grimm.txt out.txt 1000 0.4
</pre></div>
<p><em>Note: that last value is a factor that defines how many words from the traning text should be used. If you have a very large text with many different words the number of words (= number of states) can become very large. Using that parameter you can define the percentage of words to use. So setting this to 0.1 would mean that only the 10% most used words will be used to build the transition probability matrix and thus also the generated text. If you set this to 1 and the training text contains 2000 different 'words' the dimension of the resulting transition probability matrix will be 2000x2000 (too large for example if you want to look at the matrix yourself).</em></p>
<p>I hope that makes sense. If you have any questions don't hesitate to contact me (see below for details)!</p>
</div>


  
  <p class=tags>Tags: [
    
      <a href="/tags/machine-learning/">machine-learning</a>, 
      <a href="/tags/markov/">markov</a>, 
      <a href="/tags/programming/">programming</a> , 
      <a href="/tags/python/">python</a>
    ]
  
</div>
        <div class="footer">
            <p> &copy; Copyright 2011 Oliver Stollmann.
            <p> Contact me 
                <a href="http://o1iver.net/info.txt">directly</a> or via 
                <a href="http://facebook.com/stollmann">facebook</a>,   
                <a href="https://plus.google.com/117321259323890073987">google+</a>, 
                <a href="http://github.com/o1iver">github</a>, 
                <a href="http://bitbucket.org/o1iver">bitbucket</a>.
        </div>
    </div>
</body>
</html>


